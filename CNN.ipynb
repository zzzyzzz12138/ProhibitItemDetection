{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN training process\n",
    "1. Set up\n",
    "    - 1.1 Mapping the training data\n",
    "    - 1.2 preparing the training data \n",
    "2. Training precess\n",
    "    - 2.1 Prepare PyTorch dataset and the loader\n",
    "    - 2.2 Split the training and testing data\n",
    "    - 2.3 Define a Faster R-CNN model\n",
    "    - 2.4 choose the optimizer\n",
    "    - 2.5 train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category of knife:\n",
      "Straight_Knife: 809\n",
      "Folding_Knife: 1589\n",
      "Scissor: 1494\n",
      "Utility_Knife: 1635\n",
      "Multi-tool_Knife: 1612\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "annotation_dir = 'dataset/train/train_annotation'\n",
    "\n",
    "class_count = defaultdict(int)\n",
    "\n",
    "# get the class name from the annotation files\n",
    "for filename in os.listdir(annotation_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(annotation_dir, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    class_name = parts[1]  # the second column is the class name\n",
    "                    class_count[class_name] += 1\n",
    "\n",
    "print(\"category of knife:\")\n",
    "for cls, count in class_count.items():\n",
    "    print(f\"{cls}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Mapping the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of trainingset: 7139\n",
      "the first three: [('dataset/train/train_image\\\\009000.jpg', 'Straight_Knife'), ('dataset/train/train_image\\\\009002.jpg', 'Straight_Knife'), ('dataset/train/train_image\\\\009003.jpg', 'Straight_Knife')]\n"
     ]
    }
   ],
   "source": [
    "image_dir = 'dataset/train/train_image'\n",
    "anno_dir = 'dataset/train/train_annotation'\n",
    "# Example: 009000.jpg Straight_Knife 763 486 840 549\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for txt_file in os.listdir(anno_dir):\n",
    "    if txt_file.endswith('.txt'):\n",
    "        txt_path = os.path.join(anno_dir, txt_file)\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                image_name = parts[0]\n",
    "                label = parts[1]\n",
    "                image_path = os.path.join(image_dir, image_name)\n",
    "                data_list.append((image_path, label))\n",
    "\n",
    "print(f\"the number of trainingset: {len(data_list)}\")\n",
    "print(\"the first three:\", data_list[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Preparing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of trainingset: 7133 \n",
      "the first three:\n",
      "image_path: dataset/train/train_image\\009000.jpg\n",
      "category: 0\n",
      "framework: [763, 486, 840, 549]\n",
      "image_path: dataset/train/train_image\\009002.jpg\n",
      "category: 0\n",
      "framework: [650, 289, 717, 357]\n",
      "image_path: dataset/train/train_image\\009003.jpg\n",
      "category: 0\n",
      "framework: [458, 336, 580, 389]\n"
     ]
    }
   ],
   "source": [
    "labeled_data_list = []\n",
    "\n",
    "label_to_id = {\n",
    "    'Straight_Knife': 0,\n",
    "    'Folding_Knife': 1,\n",
    "    'Scissor': 2,\n",
    "    'Utility_Knife': 3,\n",
    "    'Multi-tool_Knife': 4\n",
    "}\n",
    "\n",
    "def is_valid_box(bbox, max_width=1225, max_height=954):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return (\n",
    "        x2 > x1 and y2 > y1 and\n",
    "        x1 >= 0 and y1 >= 0 and\n",
    "        x2 <= max_width and y2 <= max_height\n",
    "    )\n",
    "\n",
    "\n",
    "for txt_file in os.listdir(anno_dir):\n",
    "    if txt_file.endswith('.txt'):\n",
    "        txt_path = os.path.join(anno_dir, txt_file)\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 6:\n",
    "                    image_name = parts[0]\n",
    "                    label_name = parts[1]\n",
    "                    bbox = list(map(int, parts[2:6]))\n",
    "                    if is_valid_box(bbox):\n",
    "                        label = label_to_id[label_name]\n",
    "                        image_path = os.path.join(image_dir, image_name)\n",
    "                        labeled_data_list.append((image_path, label, bbox))\n",
    "\n",
    "print(f\"the number of trainingset: {len(labeled_data_list)} \")\n",
    "print(\"the first three:\")\n",
    "for item in labeled_data_list[:3]:\n",
    "    print(\"image_path:\", item[0])\n",
    "    print(\"category:\", item[1])\n",
    "    print(\"framework:\", item[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Prepare PyTorch dataset and the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class KnifeDataset(Dataset):\n",
    "    #def __init__(self, data_list, transform=None, resize=(400, 300)):\n",
    "    def __init__(self, data_list, transform=None, resize=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, bbox = self.data_list[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_w, orig_h = image.size  # original width and height\n",
    "\n",
    "        # resize image\n",
    "        if self.resize:\n",
    "            new_w, new_h = self.resize\n",
    "            image = image.resize((new_w, new_h))  # PIL resize\n",
    "\n",
    "            # resize bbox\n",
    "            scale_x = new_w / orig_w\n",
    "            scale_y = new_h / orig_h\n",
    "            bbox = [\n",
    "                int(bbox[0] * scale_x),\n",
    "                int(bbox[1] * scale_y),\n",
    "                int(bbox[2] * scale_x),\n",
    "                int(bbox[3] * scale_y)\n",
    "            ]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        boxes = torch.tensor([bbox], dtype=torch.float32)  # shape [1, 4]\n",
    "        labels = torch.tensor([label], dtype=torch.int64)  # shape [1]\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((400, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 split the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation sets\n",
    "dataset = KnifeDataset(labeled_data_list, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# define collate_fn to prepare for the Faster R-CNN\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Define a Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"current device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# use the pre-trained ResNet18 model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 choose the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)   # learning rate: 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 197.0167\n",
      "Epoch 2/10, Loss: 153.2321\n",
      "Epoch 3/10, Loss: 146.7032\n",
      "Epoch 4/10, Loss: 135.0985\n",
      "Epoch 5/10, Loss: 127.8599\n",
      "Epoch 6/10, Loss: 119.3529\n",
      "Epoch 7/10, Loss: 114.3767\n",
      "Epoch 8/10, Loss: 101.5684\n",
      "Epoch 9/10, Loss: 91.1660\n",
      "Epoch 10/10, Loss: 83.8986\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        try:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_idx} error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fasterrcnn_opixray.pth\")\n",
    "print(\"model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
